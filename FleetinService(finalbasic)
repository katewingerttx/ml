import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random
import math
import time
start = time.time()

# write data into correct form
df = pd.read_excel(r'C:\Users\bmpkpw\Desktop\Kate Winger\Results\FIS_Data.xlsx', sheet_name='53-ft')
array = df.to_numpy()
# Data Standardization
k = 1
for i in range(len(array)):
    array[i,0] = k # assign month
    k += 1
    if k == 13: # reset month after December
        k = 1
array[:,1] = array[:,1] / 100000
array[:,4] = array[:,4] / 10
array[:,5:14] = array[:,5:14] / 100000
array = array[0:57,:]

def getFISData(numIn, rw): # manipulate array into useful data form
    fisData = np.zeros(shape=(len(array)-rw, numIn+4), dtype=np.float32)
    for i in range(len(array)-rw):
        fisData[i,0] = math.ceil(((i+1+rw))/12) # assign year (data must start at January)
        for j in range(12):
            if array[i+rw,0] == j+1:
                fisData[i,1+j] = 1 # january = true
        for j in range(rw):
            fisData[i,j+13] = array[i+j,1] # last 12 months of loading
            fisData[i,rw+j+13] = array[i+j,12] # last 12 months of fleet in service
        fisData[i,(2*rw)+13] = array[i+rw,2] # slot utilization
        fisData[i,(2*rw)+14] = array[i+rw,3] # days per cycle
        fisData[i,(2*rw)+15] = array[i+rw,4] # days
        fisData[i,(2*rw)+16] = array[i+rw,6] # idle platforms
        fisData[i,(2*rw)+17] = array[i+rw,7] # non-ttx platforms
        fisData[i,(2*rw)+18] = array[i+rw,8] # historical prediction
        fisData[i,(2*rw)+19] = array[i+rw,12] # actual fleet in service
        
    # export to excel to check getFISData() 
    writer = pd.ExcelWriter('C:\\Users\\bmpkpw\\Desktop\\Kate Winger\\Results\\fisDataframe1.xlsx')
    data = pd.DataFrame(fisData)
    data.to_excel(writer)
    writer.save()
    
    return fisData

class NeuralNetwork:
    
    def __init__(self, numIn, numHid, numOut, seed):
        
        self.numInput = numIn # number of inputs
        self.numHidden = numHid # number of hidden nodes
        self.numOutput = numOut # number of outputs
        
        self.iNodes = np.zeros(shape=[self.numInput], dtype=np.float32) # creates input nodes
        self.hNodes = np.zeros(shape=[self.numHidden], dtype=np.float32) # creates hidden nodes
        self.oNodes = np.zeros(shape=[self.numOutput], dtype=np.float32) # creates output nodes
    
        self.ihWeights = np.zeros(shape=[self.numInput,self.numHidden], dtype=np.float32) # array of input x hidden weights
        self.hoWeights = np.zeros(shape=[self.numHidden,self.numOutput], dtype=np.float32) # array of hidden x output weights
        
        self.hBiases = np.zeros(shape=[self.numHidden], dtype=np.float32) # array of hidden node biases
        self.oBiases = np.zeros(shape=[self.numOutput], dtype=np.float32) # array of output biases
        
        self.rnd = random.Random(seed) # allows multiple instances
        self.initializeWeights()

    def setWeights(self, weights): # transform weights array into separate arrays based on where they are applied so they can be used and manipulated
            
        idx = 0
        for i in range(self.numInput):
            for j in range(self.numHidden):
                self.ihWeights[i,j] = weights[idx]
                idx += 1
                
        for j in range(self.numHidden):
            self.hBiases[j] = weights[idx]
            idx += 1
                
        for j in range(self.numHidden):
            for k in range(self.numOutput):
                self.hoWeights[j,k] = weights[idx]
                idx += 1
                
        for k in range(self.numOutput):
            self.oBiases[k] = weights[idx]
            idx += 1
                    
    def getWeights(self): # return weights/biases into one array after use and manipulation
        
        tw = self.totalWeights() # determine number of weights/biases
        result = np.zeros(shape=[tw], dtype=np.float32)
        idx = 0 # points into result
        
        for i in range(self.numInput):
            for j in range(self.numHidden):
                result[idx] = self.ihWeights[i,j]
                idx += 1
                
        for j in range(self.numHidden):
            result[idx] = self.hBiases[j]
            idx += 1
            
        for j in range(self.numHidden):
            for k in range(self.numOutput):
                result[idx] = self.hoWeights[j,k]
                idx += 1
            
        for k in range(self.numOutput):
            result[idx] = self.oBiases[k]
            idx += 1
            
        return result
        
    def initializeWeights(self): # set initial random weights to start
        
        numWts = self.totalWeights()
        wts = np.zeros(shape=[numWts], dtype=np.float32)
        lo = -0.5; hi = 0.5
        for idx in range(len(wts)):
            wts[idx] = (hi-lo) * self.rnd.random() + lo
        self.setWeights(wts)
        
    def computeOutputs(self, xValues, helper_values): # forward propagation
        
        hSums = np.zeros(shape=[self.numHidden], dtype=np.float32) # temporary sum variable
        oSums = np.zeros(shape=[self.numOutput], dtype=np.float32) # temporary sum variable
        
        for i in range(self.numInput):
            self.iNodes[i] = xValues[i] # start with current rows input values
            
        for j in range(self.numHidden):
            for i in range(self.numInput):
                hSums[j] += self.iNodes[i] * self.ihWeights[i,j] # multiply input nodes by the weight values
        
        for j in range(self.numHidden):
            hSums[j] += self.hBiases[j] # add bias values
            
        for j in range(self.numHidden):
            self.hNodes[j] = self.hypertan(hSums[j]) # run values through activation function: use tanh
            
        for k in range(self.numOutput):
            for j in range(self.numHidden):
                oSums[k] += self.hNodes[j] * self.hoWeights[j,k] # multiply hidden nodes by weight values
        
        for k in range(self.numOutput):
            oSums[k] += self.oBiases[k] + helper_values[0] - helper_values[1] # add bias values
            
        self.oNodes = oSums # not using another activation function here; previously experimented with but only useful for classification NN

        result = np.zeros(shape=[self.numOutput], dtype=np.float32)
        for k in range(self.numOutput): 
            result[k] = self.oNodes[k]

        return result

    def train(self, rw, trainData, maxEpochs, learnRate, errUpdate, momentum, resultsArray):
        
        hoGrads = np.zeros(shape=[self.numHidden, self.numOutput], dtype=np.float32) # hidden-to-output weights gradients
        obGrads = np.zeros(shape=[self.numOutput], dtype=np.float32) # output node bias gradients
        ihGrads = np.zeros(shape=[self.numInput, self.numHidden], dtype=np.float32) # input-to-hidden weights gradients
        hbGrads = np.zeros(shape=[self.numHidden], dtype=np.float32) # hidden bias gradients
        
        oSignals = np.zeros(shape=[self.numOutput], dtype=np.float32) # output signals: gradients w/o assoc. input terms
        hSignals = np.zeros(shape=[self.numHidden], dtype=np.float32) # hidden signals: gradients w/o assoc. input terms
        
        ih_prev_weights_delta = np.zeros(shape=[self.numInput, self.numHidden], dtype=np.float32) # saved delta values for momentum
        h_prev_biases_delta = np.zeros(shape=[self.numHidden], dtype=np.float32) # saved delta values for momentum
        ho_prev_weights_delta = np.zeros(shape=[self.numHidden, self.numOutput], dtype=np.float32) # saved delta values for momentum
        o_prev_biases_delta = np.zeros(shape=[self.numOutput], dtype=np.float32) # saved delta values for momentum
        
        epoch = 0
        self.numTrainItems = len(trainData)
        x_values = np.zeros(shape=[self.numInput], dtype=np.float32) # array to insert current input values
        output = np.zeros(shape=[self.numOutput], dtype=np.float32) # array to insert current output
        actual_values = np.zeros(shape=[self.numOutput], dtype=np.float32) # array to insert current actual output
        helper_values = np.zeros(shape=[2], dtype=np.float32)
        
        indices = np.arange(self.numTrainItems) # [0, 1, 2, . . n-1] 
        err = np.full(shape=[maxEpochs//errUpdate + 1], fill_value = 1000, dtype=np.float32) # array to keep track of all error values
        self.resultsCounter = 0
        
        while epoch <= maxEpochs:
            
            if epoch == 0:
                time1 = time.time() # timestamp used to estimate training time
            
            self.rnd.shuffle(indices) # scramble order of training items
            
            for ii in range(self.numTrainItems):
                idx = indices[ii]
                
                for j in range(self.numInput):
                    x_values[j] = trainData[idx,j] # get the current input values
                for j in range(self.numOutput):
                    actual_values[j] = trainData[idx,j+3+self.numInput] # get current target values
                helper_values[0] = trainData[idx,(2*rw)+16]
                helper_values[1] = trainData[idx,(2*rw)+17]
                self.computeOutputs(x_values, helper_values) # results stored internally
                
            # Backpropagation
                
                #1. compute output node signals
                for k in range(self.numOutput):
                    deriv = 1 # derivative is one because I chose not to use an activation function for the outputs
                    oSignals[k] = deriv * (self.oNodes[k] - actual_values[k]) # multiply derivative by error
                
                #2. compute hidden-to-output weight gradients using output signals
                for j in range(self.numHidden):
                    for k in range(self.numOutput):
                        hoGrads[j,k] = oSignals[k] * self.hNodes[j]
                        
                #3. compute output node bias gradients using output signals
                for k in range(self.numOutput):
                    obGrads[k] = oSignals[k] 
                    
                #4. compute hidden node signals
                for j in range(self.numHidden):
                    sum = 0.0
                    for k in range(self.numOutput):
                        sum += oSignals[k] * self.hoWeights[j,k]
                    deriv = (1 - self.hNodes[j]) * (1 + self.hNodes[j]) # derivative of tanh function used to compute output values
                    hSignals[j] = deriv * sum
                    
                #5. compute input-to-hidden weight gradients using hidden signals
                for i in range(self.numInput):
                    for j in range(self.numHidden):
                        ihGrads[i,j] = hSignals[j] * self.iNodes[i]
                        
                #6. compute hidden node bias gradients using hidden signals
                for j in range(self.numHidden):
                    hbGrads[j] = hSignals[j]
                
            # update weights and biases using the gradients and momentum
                
                #1. update input-to-hidden weights
                for i in range(self.numInput):
                    for j in range(self.numHidden):
                        delta = -1.0 * learnRate * ihGrads[i,j]
                        self.ihWeights[i,j] += delta
                        self.ihWeights[i,j] += momentum * ih_prev_weights_delta[i,j] # momentum
                        ih_prev_weights_delta[i,j] = delta # save delta for next iteration
                    
                #2. update hidden node biases
                for j in range(self.numHidden):
                    delta = -1.0 * learnRate * hbGrads[j]
                    self.hBiases[j] += delta
                    self.hBiases[j] += momentum * h_prev_biases_delta[j]
                    h_prev_biases_delta[j] = delta 
                    
                #3. update hidden-to-output weights
                for j in range(self.numHidden):
                    for k in range(self.numOutput):
                        delta = -1.0 * learnRate * hoGrads[j,k]
                        self.hoWeights[j,k] += delta
                        self.hoWeights[j,k] += momentum * ho_prev_weights_delta[j,k]
                        ho_prev_weights_delta[j,k] = delta
                    
                #4. update output node biases
                for k in range(self.numOutput):
                    delta = -1.0 * learnRate * obGrads[k]
                    self.oBiases[k] += delta
                    self.oBiases[k] += momentum * o_prev_biases_delta[k]
                    o_prev_biases_delta[k] = delta
               
            if epoch == 0:
                epochTime = (time.time() - time1) # time to complete the first epoch
                print("\nEstimated training time: %.1f minutes\n" % ((epochTime * maxEpochs) / 60))
            
            if epoch % errUpdate == 0: # run every 50 epochs 
                mape = self.MAPE(rw, trainData)
                if all(x > mape for x in err): # determine weights with least error
                    self.finalWeights = self.getWeights()
                    print("epoch = " + str(epoch) +" mape = %0.4f (best weights)" % mape)
                else:
                    print("epoch = " + str(epoch) +" mape = %0.4f " % mape)
                err[epoch//errUpdate] = mape
                resultsArray[self.resultsCounter,0] = epoch
                resultsArray[self.resultsCounter,1] = mape
                self.resultsCounter += 1
            
            # print percent complete with training
            percent = ((epoch / maxEpochs) * 100)
            percentComplete = "Percent Complete: %f%%" % percent
            print("Percent Complete: %.3f%%" % percent, end="\r")    
            
            epoch += 1
            
        # end while
        
        # print error plot
        x = np.arange(maxEpochs//errUpdate + 1)
        plt.plot(x,err)
        plt.xlabel('Epochs / %d' % errUpdate)
        plt.ylabel('MeanSquaredError')
        plt.show()
               
        result = self.getWeights()
        return result
    # end train
    
    def accuracy(self, rw, trainData, tdata, howClose, resultsArray): 
        num_correct = 0; num_wrong = 0
        self.setWeights(self.finalWeights) # use weights with the least error
        
        x_values = np.zeros(shape=[self.numInput], dtype=np.float32) # array to insert current input values
        actual_values = np.zeros(shape=[self.numOutput], dtype=np.float32) # array to insert current actual output 
        helper_values = np.zeros(shape=[2], dtype=np.float32)
        dataLength = len(trainData)
        
        for i in range(dataLength): # walk through each data item
            for j in range(self.numInput): # peel off input values from current data row
                x_values[j] = trainData[i,j]
            for j in range(self.numOutput): # peel off target values from current data row
                actual_values[j] = trainData[i,j+3+self.numInput]
            helper_values[0] = trainData[i,(2*rw)+16]
            helper_values[1] = trainData[i,(2*rw)+17]
            
            output_values = self.computeOutputs(x_values, helper_values) # computed output values
            
            if (np.absolute(output_values[0] - actual_values[0]) < (howClose*actual_values[0])): # within a certain amount to be correct
                num_correct += 1
            else:
                num_wrong += 1
                
            for k in range(12): # determine month
                if trainData[i,k+1] == 1:
                    month = k+1
            year = (trainData[i,0])+2013 # determine year
            
            print("%d\t%d:\t%.5f\t%.5f\t%2.1f %%" % (year, month, actual_values[0], output_values[0], (np.absolute((actual_values[0] - output_values[0]) / actual_values[0])*100)))
            resultsArray[self.resultsCounter,0] = year
            resultsArray[self.resultsCounter,1] = month
            resultsArray[self.resultsCounter,2] = actual_values[0]
            resultsArray[self.resultsCounter,3] = output_values[0]
            resultsArray[self.resultsCounter,4] = (np.absolute((actual_values[0] - output_values[0]) / actual_values[0])*100)
            self.resultsCounter += 1
                
            sumErrorPred = 0
            sumErrorHist = 0
                
        print("\n\n\nValidation Data: year-month-actual-prediction-error-historical-error")
        for i in range(len(tdata)):     
            for j in range(self.numInput): # peel off input values from current data row
                x_values[j] = tdata[i,j]
            for j in range(self.numOutput): # peel off target values from current data row
                actual_values[j] = tdata[i,j+3+self.numInput]
            helper_values[0] = tdata[i,(2*rw)+16]
            helper_values[1] = tdata[i,(2*rw)+17]
            output_values = self.computeOutputs(x_values, helper_values) # computed output values
            errorPred = (np.absolute((actual_values[0] - output_values[0]) / actual_values[0])*100)
            sumErrorPred += errorPred
            errorHist = (np.absolute((actual_values[0] - tdata[i,(2*rw)+18]) / actual_values[0])*100)
            sumErrorHist += errorHist
            for k in range(12): # determine month
                if tdata[i,k+1] == 1:
                    month = k+1
            year = (tdata[i,0])+2013 # determine year
            print("%d\t%d:\t%.5f\t%.5f\t%.1f %%\t%.5f\t%.1f %%" % (year, month, actual_values[0], output_values[0], errorPred, tdata[i,(2*rw)+18], errorHist))
            resultsArray[self.resultsCounter,0] = year
            resultsArray[self.resultsCounter,1] = month
            resultsArray[self.resultsCounter,2] = actual_values[0]
            resultsArray[self.resultsCounter,3] = output_values[0]
            resultsArray[self.resultsCounter,4] = errorPred
            resultsArray[self.resultsCounter,5] = tdata[i,(2*rw)+18]
            resultsArray[self.resultsCounter,6] = errorHist
            self.resultsCounter += 1
            
        impr = (sumErrorHist - sumErrorPred)
        avgImpr = impr / len(tdata)
        
        return impr, avgImpr

    def MAPE(self, rw, tdata):
        sumPercentError = 0.0
        
        x_values = np.zeros(shape=[self.numInput], dtype=np.float32) # array to insert current input values
        helper_values = np.zeros(shape=[2], dtype=np.float32)
        actual_values = np.zeros(shape=[self.numOutput], dtype=np.float32) # array to insert current actual output
        
        for ii in range(self.numTrainItems): # walk through each data item
            for jj in range(self.numInput): # peel off input values from current data row
                x_values[jj] = tdata[ii,jj]
            for jj in range(self.numOutput): # peel of target values from current data row
                actual_values[jj] = tdata[ii,jj+3+self.numInput]
            helper_values[0] = tdata[ii,(2*rw)+16]
            helper_values[1] = tdata[ii,(2*rw)+17]
                
            output_values = self.computeOutputs(x_values, helper_values) # computed output values
                                 
            for j in range(self.numOutput):
                err = np.absolute(actual_values[j] - output_values[j]) / actual_values[j]
                sumPercentError += err
                
        return sumPercentError / self.numTrainItems
    
    def hypertan(self,x):
        if x < -20.0:
            return -1.0
        elif x > 20.0:
            return 1.0
        else:
            return math.tanh(x)
    
    def totalWeights(self):
        tw = (self.numInput * self.numHidden) + (self.numHidden * self.numOutput) + self.numHidden + self.numOutput
        return tw

# end class NeuralNetwork

def main():
    sizeRollingWindow = 1 # number of months of historical fleet data
    numIn = (2*sizeRollingWindow) + 16 # number of inputs
    numHid = 45 # number of neurons in hidden layer
    numOut = 1 # number of outputs (predict next mileage number)

    fisData = getFISData(numIn, sizeRollingWindow)
    lenData = len(fisData)
       
    sizeTestData = 9 # choose how many rows of data will used for testing and not training
    trainFISData = fisData[0:(lenData-sizeTestData),:] # split data into train and test
    testFISData = fisData[(lenData-sizeTestData):lenData,:]
    lenTestData = len(testFISData)
    np.set_printoptions(formatter = {'float': '{: 0.5f}'.format})
    
    print("\nCreating a %d-%d-%d neural network " % (numIn, numHid, numOut))
    nn = NeuralNetwork(numIn, numHid, numOut, seed=0)
    
    maxEpochs = 2000 # number of interations
    learnRate = .01 # how quickly the program trains
    errUpdate = 50 # how often to print an error update
    momentum = 0 # momentum value
    print("\nSetting maxEpochs = " + str(maxEpochs))
    print("Setting learning rate = %0.3f " % learnRate)
    
    length = math.floor((maxEpochs/errUpdate) + len(trainFISData) + len(testFISData) + 1)
    resultsArray = np.zeros(shape=[length,7], dtype=np.float32) # array to insert current input values
    
    print("\nStarting training...")
    nn.train(sizeRollingWindow, trainFISData, maxEpochs, learnRate, errUpdate, momentum, resultsArray)
    print("Training complete. \n")
    
    print("First few month-actual-predicted: ")
    howClose = .02 # within what percentage of error is considered correct/acceptable
    impr, avgImpr = nn.accuracy(sizeRollingWindow, trainFISData, testFISData, howClose, resultsArray)
    
    # export results to excel 
    writer = pd.ExcelWriter('C:\\Users\\bmpkpw\\Desktop\\Kate Winger\\Results\\fisDataResults1.xlsx')
    data = pd.DataFrame(resultsArray)
    data.to_excel(writer)
    writer.save()
    
    print("\nSum of Percent Error Improvement on %d-item validation data: %0.2f%%" % (lenTestData, impr))
    print("\nAverage Monthly Percent Error Improvement: %0.2f%%" % avgImpr)
    print("\nEnd \n")
    
        
    # end of code      
main()
print("Run Time: %.1f minutes" % ((time.time() - start) / 60))

